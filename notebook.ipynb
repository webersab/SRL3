{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "import allennlp_models.tagging\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_between_brackets(input_string):\n",
    "    # Define the regex pattern for text between brackets\n",
    "    pattern = r'\\[(.*?)\\]'\n",
    "    \n",
    "    # Use re.findall to find all occurrences of the pattern\n",
    "    results = re.findall(pattern, input_string)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Word_Index Vocab_Dict \n",
    "\n",
    "def build_word_index(tokenized_doc):\n",
    "    word_idx = {}\n",
    "    for idx, word in enumerate(tokenized_doc):\n",
    "        word_idx[idx] = word\n",
    "    print(word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lematizing\n",
    "\n",
    "def lemmatize(word, nlp):\n",
    "    doc = nlp(word)\n",
    "    return doc[0].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_strings(strings, seq1, seq2):\n",
    "    # Return a new list containing only the strings that do not contain the sequence.\n",
    "    return [s for s in strings if seq1 not in s and seq2 not in s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_number(num, list_of_lists):\n",
    "    return any(num in sublist for sublist in list_of_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRL_MODEL_PATH = \"srl.tar.gz\"\n",
    "COREF_MODEL_PATH = \"coref.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "predictor_srl = Predictor.from_path(SRL_MODEL_PATH)\n",
    "predictor_coref = Predictor.from_path(COREF_MODEL_PATH)\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOCUMENT=\"Marley was dead, to begin with. There is no doubt whatever about that. The register of his burial was signed by the clergyman, the clerk, the undertaker, and the chief mourner. Scrooge signed it. And Scrooge\\’s name was good upon \\’Change for anything he chose to put his hand to. Old Marley was as dead as a door-nail.\"\n",
    "#DOCUMENT = \"Once upon a time there were four little rabbits, and their names were Flopsy, Mopsy, Cotton-tail and Peter. They lived with their mother in a sand-bank, underneath the root of a very big fir tree. \\\"Now, my dears,\\\" said old Mrs. Rabbit one morning, \\\"You may go into the fields or down the lane, but don't go into Mr. McGregor's garden. Your father had an accident there; he was put in a pie by Mrs. McGregor.\\\" Now run along and don\\'t get into mischief. I am going out.\\\" Then old Mrs. Rabbit took a basket and her umbrella and went through the wood to the baker\\'s.\"\n",
    "DOCUMENT = \"Chris Mitchell left the house and went to the garden. He met Melissa Smith and proceeded to climb the wall. Next the man jumped into the rose bush and she watched him.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file read\n"
     ]
    }
   ],
   "source": [
    "with open('a_little_princess.txt', 'r') as file:\n",
    "    content = file.read(500)\n",
    "print(\"file read\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-ARG0', 'I-ARG0', 'B-V', 'B-ARG1', 'I-ARG1', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['B-ARG0', 'I-ARG0', 'O', 'O', 'O', 'O', 'B-V', 'B-ARG4', 'I-ARG4', 'I-ARG4', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG0', 'B-V', 'B-ARG1', 'I-ARG1', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG0', 'O', 'O', 'O', 'O', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG0', 'O', 'O', 'O', 'O', 'O', 'O', 'B-V', 'B-ARG1', 'I-ARG1', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARGM-TMP', 'B-ARG0', 'I-ARG0', 'B-V', 'B-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'O', 'O', 'O', 'O', 'O']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG0', 'B-V', 'B-ARG1', 'O']\n",
      "['ARG0: Chris Mitchell', 'V: left', 'ARG1: the house']\n",
      "['ARG0: Chris Mitchell', 'V: went', 'ARG4: to the garden']\n",
      "['ARG0: He', 'V: met', 'ARG1: Melissa Smith']\n",
      "['ARG0: He', 'V: proceeded', 'ARG1: to climb the wall']\n",
      "['ARG0: He', 'V: climb', 'ARG1: the wall']\n",
      "['ARGM-TMP: Next', 'ARG0: the man', 'V: jumped', 'ARGM-DIR: into the rose bush']\n",
      "['ARG0: she', 'V: watched', 'ARG1: him']\n",
      "{'verbs': [{'verb': 'left', 'description': '[ARG0: Chris Mitchell] [V: left] [ARG1: the house] and went to the garden . He met Melissa Smith and proceeded to climb the wall . Next the man jumped into the rose bush and she watched him .', 'tags': ['B-ARG0', 'I-ARG0', 'B-V', 'B-ARG1', 'I-ARG1', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'verb': 'went', 'description': '[ARG0: Chris Mitchell] left the house and [V: went] [ARG4: to the garden] . He met Melissa Smith and proceeded to climb the wall . Next the man jumped into the rose bush and she watched him .', 'tags': ['B-ARG0', 'I-ARG0', 'O', 'O', 'O', 'O', 'B-V', 'B-ARG4', 'I-ARG4', 'I-ARG4', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'verb': 'met', 'description': 'Chris Mitchell left the house and went to the garden . [ARG0: He] [V: met] [ARG1: Melissa Smith] and proceeded to climb the wall . Next the man jumped into the rose bush and she watched him .', 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG0', 'B-V', 'B-ARG1', 'I-ARG1', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'verb': 'proceeded', 'description': 'Chris Mitchell left the house and went to the garden . [ARG0: He] met Melissa Smith and [V: proceeded] [ARG1: to climb the wall] . Next the man jumped into the rose bush and she watched him .', 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG0', 'O', 'O', 'O', 'O', 'B-V', 'B-ARG1', 'I-ARG1', 'I-ARG1', 'I-ARG1', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'verb': 'climb', 'description': 'Chris Mitchell left the house and went to the garden . [ARG0: He] met Melissa Smith and proceeded to [V: climb] [ARG1: the wall] . Next the man jumped into the rose bush and she watched him .', 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG0', 'O', 'O', 'O', 'O', 'O', 'O', 'B-V', 'B-ARG1', 'I-ARG1', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}, {'verb': 'jumped', 'description': 'Chris Mitchell left the house and went to the garden . He met Melissa Smith and proceeded to climb the wall . [ARGM-TMP: Next] [ARG0: the man] [V: jumped] [ARGM-DIR: into the rose bush] and she watched him .', 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARGM-TMP', 'B-ARG0', 'I-ARG0', 'B-V', 'B-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'I-ARGM-DIR', 'O', 'O', 'O', 'O', 'O']}, {'verb': 'watched', 'description': 'Chris Mitchell left the house and went to the garden . He met Melissa Smith and proceeded to climb the wall . Next the man jumped into the rose bush and [ARG0: she] [V: watched] [ARG1: him] .', 'tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ARG0', 'B-V', 'B-ARG1', 'O']}], 'words': ['Chris', 'Mitchell', 'left', 'the', 'house', 'and', 'went', 'to', 'the', 'garden', '.', 'He', 'met', 'Melissa', 'Smith', 'and', 'proceeded', 'to', 'climb', 'the', 'wall', '.', 'Next', 'the', 'man', 'jumped', 'into', 'the', 'rose', 'bush', 'and', 'she', 'watched', 'him', '.']}\n"
     ]
    }
   ],
   "source": [
    "predictions_srl = predictor_srl.predict(sentence=DOCUMENT)\n",
    "\n",
    "for j in predictions_srl[\"verbs\"]:\n",
    "    print(j['tags'])\n",
    "\n",
    "for i in predictions_srl[\"verbs\"]:\n",
    "    print(extract_between_brackets(i[\"description\"]))\n",
    "\n",
    "print(predictions_srl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'top_spans': [[0, 1], [3, 4], [8, 9], [11, 11], [12, 12], [13, 14], [16, 16], [18, 18], [19, 20], [23, 24], [25, 25], [27, 29], [31, 31], [33, 33]], 'antecedent_indices': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]], 'predicted_antecedents': [-1, -1, -1, 0, -1, -1, -1, -1, -1, 3, -1, -1, 5, 9], 'document': ['Chris', 'Mitchell', 'left', 'the', 'house', 'and', 'went', 'to', 'the', 'garden', '.', 'He', 'met', 'Melissa', 'Smith', 'and', 'proceeded', 'to', 'climb', 'the', 'wall', '.', 'Next', 'the', 'man', 'jumped', 'into', 'the', 'rose', 'bush', 'and', 'she', 'watched', 'him', '.'], 'clusters': [[[0, 1], [11, 11], [23, 24], [33, 33]], [[13, 14], [31, 31]]]}\n"
     ]
    }
   ],
   "source": [
    "predictions_coref = predictor_coref.predict(document=DOCUMENT)\n",
    "print(predictions_coref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Chris Mitchell, Type: PERSON, Start Token: 0, End Token: 2\n",
      "Entity: Melissa Smith, Type: PERSON, Start Token: 13, End Token: 15\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(DOCUMENT)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"Entity: {ent.text}, Type: {ent.label_}, Start Token: {ent.start}, End Token: {ent.end}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Chris Mitchell', 1: 'Chris Mitchell', 11: 'Chris Mitchell', 23: 'Chris Mitchell', 24: 'Chris Mitchell', 33: 'Chris Mitchell', 13: 'Melissa Smith', 14: 'Melissa Smith', 31: 'Melissa Smith'}\n"
     ]
    }
   ],
   "source": [
    "protagonist_dict = {}\n",
    "\n",
    "for idx, i in enumerate(predictions_coref[\"clusters\"]):\n",
    "    for ent in doc.ents:\n",
    "        if contains_number(ent.start,i):\n",
    "             for sublist in i:\n",
    "                for number in sublist:\n",
    "                # Assign the specific string to the number in the dictionary\n",
    "                    protagonist_dict[number] = ent.text\n",
    "print(protagonist_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arg1_value(string_list):\n",
    "    for string in string_list:\n",
    "        # Check if the string contains 'ARG1:'\n",
    "        if \"ARG1:\" in string:\n",
    "            # Split the string on 'ARG1:' and return the part after it\n",
    "            return string.split(\"ARG1:\", 1)[1].strip()  # Strip to remove leading/trailing whitespace\n",
    "    return None  # Return None if no such string is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_string_position(string_list, target_string):\n",
    "    try:\n",
    "        # Find the index of the target string in the list\n",
    "        position = string_list.index(target_string)\n",
    "        return position\n",
    "    except ValueError:\n",
    "        # Return -1 if the string is not found\n",
    "        return -1\n",
    "\n",
    "def check_for_matches(protagonist_dict, n, tag):\n",
    "    position = find_string_position(n['tags'], tag)\n",
    "    if position==-1:\n",
    "        return '0'\n",
    "    else:\n",
    "        if position in protagonist_dict.keys():\n",
    "            return protagonist_dict[position]\n",
    "        else:\n",
    "            return get_arg1_value(extract_between_brackets(n[\"description\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "Chris Mitchell PERSON\n",
      "[0, 1] 35\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "lemmatize() missing 1 required positional argument: 'nlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mARG\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m n[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m'\u001b[39m][j[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m#print(extract_between_brackets(n[\"description\"]),n['tags'][j[0]], lemmatize(n['verb']))\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mARG0\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m n[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m'\u001b[39m][j[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28mprint\u001b[39m(protagonist, \u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mverb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, check_for_matches(protagonist_dict, n, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB-ARG1\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mARG1\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m n[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m'\u001b[39m][j[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28mprint\u001b[39m(check_for_matches(protagonist_dict, n, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB-ARG0\u001b[39m\u001b[38;5;124m\"\u001b[39m), lemmatize(n[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverb\u001b[39m\u001b[38;5;124m'\u001b[39m]), protagonist)\n",
      "\u001b[0;31mTypeError\u001b[0m: lemmatize() missing 1 required positional argument: 'nlp'"
     ]
    }
   ],
   "source": [
    "for idx, i in enumerate(predictions_coref[\"clusters\"]):\n",
    "    #print(f\"Entity{idx}: {i}\")\n",
    "    protagonist = \"0\"\n",
    "    for ent in doc.ents:\n",
    "        if contains_number(ent.start,i):\n",
    "            protagonist = ent.text\n",
    "            print(\"--------\")\n",
    "            print(ent.text, ent.label_)\n",
    "    for j in i:\n",
    "        #print(predictions_srl['words'][j[0]])\n",
    "        for n in predictions_srl['verbs']:\n",
    "            print(j,len(n['tags']))\n",
    "            if 'ARG' in n['tags'][j[0]]:\n",
    "                #print(extract_between_brackets(n[\"description\"]),n['tags'][j[0]], lemmatize(n['verb']))\n",
    "                if 'ARG0' in n['tags'][j[0]]:\n",
    "                    print(protagonist, lemmatize(n['verb']), check_for_matches(protagonist_dict, n, \"B-ARG1\"))\n",
    "                elif 'ARG1' in n['tags'][j[0]]:\n",
    "                    print(check_for_matches(protagonist_dict, n, \"B-ARG0\"), lemmatize(n['verb']), protagonist)\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

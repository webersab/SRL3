{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n",
      "{'pipeline': 'entity,quote,supersense,event,coref', 'model': 'big'}\n",
      "--- startup: 8.724 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from booknlp.booknlp import BookNLP\n",
    "\n",
    "model_params={\n",
    "\t\t\"pipeline\":\"entity,quote,supersense,event,coref\", \n",
    "\t\t\"model\":\"big\"\n",
    "\t}\n",
    "\t\n",
    "booknlp=BookNLP(\"en\", model_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text saved to winnie_the_pooh_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_gutenberg_text(input_path, output_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # 2. Remove page numbers or headers like [Page 1]\n",
    "    text = re.sub(r'\\[Page \\d+\\]', '', text)\n",
    "\n",
    "    # 3. Normalize whitespace\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    # Merge broken lines (assume that a line ending with a lowercase letter continues)\n",
    "    text = re.sub(r'(?<=[a-z,;])\\n(?=[a-zA-Z])', ' ', text)\n",
    "    # Normalize multiple blank lines to exactly two (paragraph break)\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)\n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    # 4. Fix Unicode characters (basic replacements)\n",
    "    text = text.replace('“', '\"').replace('”', '\"').replace('‘', \"'\").replace('’', \"'\")\n",
    "    text = text.replace('—', '-').replace('–', '-')  # em dash and en dash to hyphen\n",
    "    text = text.replace('\\r', '')  # Remove carriage returns if any\n",
    "    text = text.replace('_','')\n",
    "\n",
    "    # 6. Write cleaned text\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "\n",
    "    print(f\"Cleaned text saved to {output_path}\")\n",
    "\n",
    "\n",
    "clean_gutenberg_text(\n",
    "    input_path='winnie_the_pooh.txt',\n",
    "    output_path='winnie_the_pooh_cleaned.txt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- spacy: 2.433 seconds ---\n",
      "--- entities: 16.742 seconds ---\n",
      "--- quotes: 0.016 seconds ---\n",
      "--- attribution: 64.589 seconds ---\n",
      "--- name coref: 0.018 seconds ---\n",
      "--- coref: 18.497 seconds ---\n",
      "--- TOTAL (excl. startup): 102.343 seconds ---, 30755 words\n"
     ]
    }
   ],
   "source": [
    "# Input file to process\n",
    "input_file=\"winnie_the_pooh_cleaned.txt\"\n",
    "\n",
    "# Output directory to store resulting files in\n",
    "output_directory=\"./output_dir/winnie/\"\n",
    "\n",
    "# File within this directory will be named ${book_id}.entities, ${book_id}.tokens, etc.\n",
    "book_id=\"winnie_the_pooh\"\n",
    "\n",
    "booknlp.process(input_file, output_directory, book_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'w': 'rich', 'i': 576}, {'w': 'old', 'i': 815}, {'w': 'big', 'i': 908}, {'w': 'companion', 'i': 1136}, {'w': 'fun', 'i': 1543}, {'w': 'child', 'i': 1794}, {'w': 'beautiful', 'i': 1805}, {'w': 'child', 'i': 1853}, {'w': 'wolf', 'i': 2312}, {'w': 'fond', 'i': 2433}, {'w': 'intimate', 'i': 3109}, {'w': 'doll', 'i': 3224}, {'w': 'large', 'i': 3229}, {'w': 'mother', 'i': 3394}, {'w': 'thing', 'i': 3675}, {'w': 'pupil', 'i': 4286}, {'w': 'interested', 'i': 4618}, {'w': 'child', 'i': 4739}, {'w': 'person', 'i': 5076}, {'w': 'girl', 'i': 5425}, {'w': 'older', 'i': 5455}, {'w': 'punctilious', 'i': 5458}, {'w': 'baby', 'i': 5561}, {'w': 'determined', 'i': 5760}, {'w': 'fond', 'i': 5842}, {'w': 'boy', 'i': 7007}, {'w': 'pretty', 'i': 7438}, {'w': 'clever', 'i': 7977}, {'w': 'glad', 'i': 9473}, {'w': 'kind', 'i': 9526}, {'w': 'guest', 'i': 9565}, {'w': 'girl', 'i': 9577}, {'w': 'child', 'i': 9589}, {'w': 'child', 'i': 9614}, {'w': 'uncomfortable', 'i': 9670}, {'w': 'unhappy', 'i': 9672}, {'w': 'person', 'i': 9799}, {'w': 'child', 'i': 9960}, {'w': 'child', 'i': 9971}, {'w': 'leader', 'i': 10227}, {'w': 'grand', 'i': 10273}, {'w': 'bit', 'i': 10279}, {'w': 'grand', 'i': 10555}, {'w': 'soul', 'i': 10563}, {'w': 'person', 'i': 10618}, {'w': 'person', 'i': 10917}, {'w': 'child', 'i': 11494}, {'w': 'clever', 'i': 11528}, {'w': 'sure', 'i': 12344}, {'w': 'creature', 'i': 12401}, {'w': 'sorry', 'i': 12770}, {'w': 'mamma', 'i': 12839}, {'w': 'girl', 'i': 12854}, {'w': 'mother', 'i': 12984}, {'w': 'pupil', 'i': 13016}, {'w': 'people', 'i': 13341}, {'w': 'cleaner', 'i': 13616}, {'w': 'railings', 'i': 13627}, {'w': 'afraid', 'i': 13639}, {'w': 'interested', 'i': 13690}, {'w': 'mamma', 'i': 14188}, {'w': 'kinder', 'i': 14303}, {'w': 'thing', 'i': 14393}, {'w': 'old', 'i': 14453}, {'w': 'timid', 'i': 14479}, {'w': 'creature', 'i': 15180}, {'w': 'glad', 'i': 15416}, {'w': 'culprit', 'i': 15952}, {'w': 'girl', 'i': 16092}, {'w': 'afraid', 'i': 16828}, {'w': 'rich', 'i': 17487}, {'w': 'ridiculous', 'i': 17492}, {'w': 'ridiculous', 'i': 17501}, {'w': 'princess', 'i': 17621}, {'w': 'thin', 'i': 17677}, {'w': 'princess', 'i': 17731}, {'w': 'beggar', 'i': 17736}, {'w': 'mamma', 'i': 18569}, {'w': 'angel', 'i': 18719}, {'w': 'fond', 'i': 18725}, {'w': 'fond', 'i': 18897}, {'w': 'princess', 'i': 19034}, {'w': 'princess', 'i': 19041}, {'w': 'able', 'i': 20001}, {'w': 'quaint', 'i': 20499}, {'w': 'sure', 'i': 20549}, {'w': 'sure', 'i': 20615}, {'w': 'old', 'i': 22312}, {'w': 'older', 'i': 22353}, {'w': 'heiress', 'i': 22357}, {'w': 'beggar', 'i': 23295}, {'w': 'poor', 'i': 26515}, {'w': 'poor', 'i': 27571}, {'w': 'beggar', 'i': 27578}, {'w': 'stupid', 'i': 27654}, {'w': 'princess', 'i': 27943}, {'w': 'child', 'i': 28063}, {'w': 'kind', 'i': 28325}, {'w': 'kind', 'i': 28335}, {'w': 'princess', 'i': 29111}, {'w': 'older', 'i': 29992}, {'w': 'maid', 'i': 30181}, {'w': 'maid', 'i': 30189}, {'w': 'girl', 'i': 30554}, {'w': 'queerer', 'i': 30574}, {'w': 'concern', 'i': 30770}, {'w': 'stupefied', 'i': 31090}, {'w': 'years', 'i': 31192}, {'w': 'older', 'i': 31194}, {'w': 'unhappy', 'i': 31661}, {'w': 'guilty', 'i': 31673}, {'w': 'happy', 'i': 31726}, {'w': 'awkward', 'i': 31779}, {'w': 'stupid', 'i': 31789}, {'w': 'surprised', 'i': 32166}, {'w': 'startled', 'i': 32274}, {'w': 'frightened', 'i': 32279}, {'w': 'different', 'i': 32528}, {'w': 'nicer', 'i': 32762}, {'w': 'able', 'i': 33043}, {'w': 'prisoner', 'i': 33101}, {'w': 'nice', 'i': 33270}, {'w': 'poor', 'i': 33433}, {'w': 'poor', 'i': 33491}, {'w': 'aghast', 'i': 33799}, {'w': 'sparrow', 'i': 34582}, {'w': 'able', 'i': 34888}, {'w': 'child', 'i': 35583}, {'w': 'fascinated', 'i': 35657}, {'w': 'thing', 'i': 35851}, {'w': 'trap', 'i': 35925}, {'w': 'interested', 'i': 36318}, {'w': 'afraid', 'i': 36794}, {'w': 'queer', 'i': 37217}, {'w': 'nice', 'i': 37222}, {'w': 'queer', 'i': 37230}, {'w': 'fond', 'i': 38500}, {'w': 'about', 'i': 38637}, {'w': 'hungry', 'i': 39018}, {'w': 'kind', 'i': 39752}, {'w': 'orphan', 'i': 39778}, {'w': 'beggar', 'i': 39785}, {'w': 'surprised', 'i': 41026}, {'w': 'little', 'i': 41459}, {'w': 'sure', 'i': 41477}, {'w': 'glad', 'i': 41805}, {'w': 'pleasure', 'i': 43664}, {'w': 'young', 'i': 44660}, {'w': 'older', 'i': 44805}, {'w': 'princess', 'i': 44936}, {'w': 'princess', 'i': 44946}, {'w': 'princess', 'i': 45280}, {'w': 'thing', 'i': 45294}, {'w': 'princess', 'i': 45778}, {'w': 'princess', 'i': 46011}, {'w': 'princess', 'i': 46135}, {'w': 'fond', 'i': 46336}, {'w': 'anxious', 'i': 46470}, {'w': 'sure', 'i': 46597}, {'w': 'fond', 'i': 46614}, {'w': 'sorry', 'i': 47007}, {'w': 'alone', 'i': 49499}, {'w': 'right', 'i': 50188}, {'w': 'princess', 'i': 50257}, {'w': 'fairy', 'i': 50271}, {'w': 'cold', 'i': 50496}, {'w': 'hungry', 'i': 50498}, {'w': 'tired', 'i': 50500}, {'w': 'sick', 'i': 51484}, {'w': 'princess', 'i': 51493}, {'w': 'princess', 'i': 51505}, {'w': 'hungry', 'i': 51893}, {'w': 'starving', 'i': 52240}, {'w': 'alive', 'i': 53200}, {'w': 'convenience', 'i': 55934}, {'w': 'afraid', 'i': 56059}, {'w': 'tired', 'i': 56168}, {'w': 'pale', 'i': 56503}, {'w': 'tired', 'i': 56509}, {'w': 'sure', 'i': 56562}, {'w': 'sorry', 'i': 56598}, {'w': 'afraid', 'i': 56630}, {'w': 'thinner', 'i': 58529}, {'w': 'child', 'i': 58574}, {'w': 'angry', 'i': 59040}, {'w': 'hungry', 'i': 59574}, {'w': 'hungry', 'i': 59620}, {'w': 'serious', 'i': 61012}, {'w': 'girl', 'i': 62174}, {'w': 'princess', 'i': 62371}, {'w': 'equal', 'i': 65534}, {'w': 'baby', 'i': 66246}, {'w': 'puzzle', 'i': 66734}, {'w': 'savage', 'i': 67615}, {'w': 'cold', 'i': 68149}, {'w': 'fairy', 'i': 68202}, {'w': 'warm', 'i': 68761}, {'w': 'princess', 'i': 69066}, {'w': 'queer', 'i': 70080}, {'w': 'thankful', 'i': 70346}, {'w': 'glad', 'i': 71320}, {'w': 'startled', 'i': 73564}, {'w': 'pupil', 'i': 73710}, {'w': 'pupil', 'i': 73724}, {'w': 'child', 'i': 74310}, {'w': 'near', 'i': 74850}, {'w': 'near', 'i': 74864}, {'w': 'child', 'i': 74922}, {'w': 'girl', 'i': 74930}, {'w': 'glad', 'i': 75407}, {'w': 'boarder', 'i': 76600}, {'w': 'child', 'i': 76809}, {'w': 'princess', 'i': 78423}, {'w': 'more', 'i': 78435}, {'w': 'glad', 'i': 79681}, {'w': 'GLAD', 'i': 79691}, {'w': 'princess', 'i': 80441}, {'w': 'better', 'i': 80885}, {'w': 'happier', 'i': 80900}, {'w': 'glad', 'i': 81582}, {'w': 'one', 'i': 81605}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "def proc(filename):\n",
    "    with open(filename) as file:\n",
    "        data=json.load(file)\n",
    "    return data\n",
    "\n",
    "data=proc(\"./output_dir/princess/princess.book\")\n",
    "#This will print the first character's mod list. No idea what the i is, but it could be token id \n",
    "print(data['characters'][0]['mod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sara rich\n",
      "Miss Minchin child\n",
      "Ermengarde trial\n",
      "Becky heroine\n",
      "Lottie quiet\n",
      "Mr. Carrisford interested\n",
      "Emily doll\n",
      "Ram Dass near\n",
      "Captain Crewe young\n",
      "Lavinia friends\n",
      "Mr. Carmichael SURE\n",
      "Mr. Barrow businessman\n",
      "Jessie friends\n",
      "Janet sure\n",
      "Mariette pleased\n",
      "Guy Clarence fellow\n",
      "Melchisedec N/A\n",
      "Monsieur Dufarge sorry\n",
      "Donald thing\n",
      "Madame Pascal glad\n",
      "Sahib ill\n",
      "Miss St. John N/A\n",
      "Mr. Carrrisford desperate\n",
      "Mrs. Carmichael N/A\n",
      "Papa soldier\n",
      "The Last Doll fun\n",
      "Poor Miss Amelia goose\n",
      "Nora fond\n",
      "Missee Sahib N/A\n",
      "James N/A\n",
      "Miss Crewe N/A\n",
      "Mr. St. John N/A\n",
      "little Miss Legh N/A\n",
      "Monkey N/A\n",
      "Lady Meredith N/A\n",
      "Isobel N/A\n",
      "Eliza slow\n",
      "Little Missus N/A\n",
      "Rosalind Gladys N/A\n",
      "Mr. Montmorency N/A\n",
      "Russians able\n",
      "The Mermaids N/A\n",
      "the Montmorencys N/A\n",
      "Veronica Eustacia N/A\n",
      "the Lascar master\n",
      "Tom N/A\n",
      "Ermie N/A\n",
      "Anne N/A\n",
      "Messrs. Barrow & Skipworth responsible\n",
      "Lady Pitkin N/A\n",
      "the Bastille N/A\n",
      "Rats N/A\n",
      "Nature N/A\n",
      "Miss ' Meliar N/A\n",
      "Sister N/A\n",
      "Alfred the Great N/A\n",
      "Ralph Crewe N/A\n",
      "Miss Ermengarde N/A\n",
      "Uncle Tom N/A\n"
     ]
    }
   ],
   "source": [
    "def get_counter_from_dependency_list(dep_list):\n",
    "    counter=Counter()\n",
    "    for token in dep_list:\n",
    "        term=token[\"w\"]\n",
    "        tokenGlobalIndex=token[\"i\"]\n",
    "        counter[term]+=1\n",
    "    return counter\n",
    "\n",
    "for character in data[\"characters\"]:\n",
    "\n",
    "    agentList=character[\"agent\"]\n",
    "    patientList=character[\"patient\"]\n",
    "    possList=character[\"poss\"]\n",
    "    modList=character[\"mod\"]\n",
    "\n",
    "    character_id=character[\"id\"]\n",
    "    count=character[\"count\"]\n",
    "\n",
    "    referential_gender_distribution=referential_gender_prediction=\"unknown\"\n",
    "\n",
    "    if character[\"g\"] is not None and character[\"g\"] != \"unknown\":\n",
    "        referential_gender_distribution=character[\"g\"][\"inference\"]\n",
    "        referential_gender=character[\"g\"][\"argmax\"]\n",
    "\n",
    "    mentions=character[\"mentions\"]\n",
    "    proper_mentions=mentions[\"proper\"]\n",
    "    max_proper_mention=\"\"\n",
    "\n",
    "    # just print out information about named characters\n",
    "    if len(mentions[\"proper\"]) > 0:\n",
    "        max_proper_mention=mentions[\"proper\"][0][\"n\"]\n",
    "\n",
    "        print(character_id, count, max_proper_mention, referential_gender)\n",
    "\n",
    "        print()\n",
    "        printTop=100\n",
    "        for k, v in get_counter_from_dependency_list(possList).most_common(printTop):\n",
    "            print(\"\\tposs\\t%s %s\" % (v,k))\n",
    "        print()\n",
    "        for k, v in get_counter_from_dependency_list(agentList).most_common(printTop):\n",
    "            print(\"\\tagent\\t%s %s\" % (v,k))\n",
    "        print()\n",
    "        for k, v in get_counter_from_dependency_list(patientList).most_common(printTop):\n",
    "            print(\"\\tpatient\\t%s %s\" % (v,k))\n",
    "        print()\n",
    "        for k, v in get_counter_from_dependency_list(modList).most_common(printTop):\n",
    "            print(\"\\tmod\\t%s %s\" % (v,k))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tokens = pd.read_csv(\"./output_dir/winnie/winnie_the_pooh.tokens\", delimiter=\"\\t\")\n",
    "df_entities = pd.read_csv(\"./output_dir/winnie/winnie_the_pooh.entities\", delimiter=\"\\t\")\n",
    "\n",
    "#create dictionary with the shape [sentence_ID][token_ID_within_sentence][word]\n",
    "sentence_and_token_ID_to_word_dict = {}\n",
    "for character in data[\"characters\"]:\n",
    "    character_id=character[\"id\"]\n",
    "    mentions=character[\"mentions\"]\n",
    "    proper_mentions=mentions[\"proper\"]\n",
    "    if len(mentions[\"proper\"]) > 0:\n",
    "        max_proper_mention=mentions[\"proper\"][0][\"n\"]\n",
    "    else:\n",
    "        max_proper_mention = character_id\n",
    "\n",
    "    unique_start_token_ids = df_entities.loc[df_entities['COREF'] == character_id, 'start_token']\n",
    "    #print(unique_start_token_ids)\n",
    "    sentence_ids = df_tokens[df_tokens['token_ID_within_document'].isin(unique_start_token_ids)][['token_ID_within_document', 'token_ID_within_sentence', 'sentence_ID']]\n",
    "    sentence_ids = sentence_ids.reset_index(drop=True)\n",
    "\n",
    "    for index, row in sentence_ids.iterrows():\n",
    "        sentence_ID = row['sentence_ID']\n",
    "        token_ID = row['token_ID_within_sentence']\n",
    "        combined_sentence_and_token_ID = str(sentence_ID) +\":\"+ str(token_ID)\n",
    "        sentence_and_token_ID_to_word_dict[combined_sentence_and_token_ID] = max_proper_mention\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1101     8141\n",
      "1105     8162\n",
      "1112     8205\n",
      "1121     8307\n",
      "1122     8311\n",
      "        ...  \n",
      "4129    29144\n",
      "4133    29165\n",
      "4141    29200\n",
      "4150    29283\n",
      "4231    29764\n",
      "Name: start_token, Length: 143, dtype: int64\n",
      "     token_ID_within_document  token_ID_within_sentence  sentence_ID\n",
      "0                        8141                         9          397\n",
      "1                        8162                        18          398\n",
      "2                        8205                         1          401\n",
      "3                        8307                        22          405\n",
      "4                        8311                        26          405\n",
      "..                        ...                       ...          ...\n",
      "138                     29144                         8         1567\n",
      "139                     29165                        15         1568\n",
      "140                     29200                        23         1570\n",
      "141                     29283                        21         1576\n",
      "142                     29764                         7         1597\n",
      "\n",
      "[143 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#the number sets which character we are looking at\n",
    "character = data[\"characters\"][7]\n",
    "character_id=character[\"id\"]\n",
    "\n",
    "#get all sentencs where COREF = character_id\n",
    "unique_start_token_ids = df_entities.loc[df_entities['COREF'] == character_id, 'start_token']\n",
    "print(unique_start_token_ids)\n",
    "sentence_ids = df_tokens[df_tokens['token_ID_within_document'].isin(unique_start_token_ids)][['token_ID_within_document', 'token_ID_within_sentence', 'sentence_ID']]\n",
    "sentence_ids = sentence_ids.reset_index(drop=True)\n",
    "print(sentence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "SRL_MODEL_PATH = \"srl.tar.gz\"\n",
    "predictor_srl = Predictor.from_path(SRL_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_arg(n, arg_name, dict_name, sentence_ID):\n",
    "    if arg_name in n[\"tags\"]:\n",
    "        index = n['tags'].index(arg_name)\n",
    "        if str(sentence_ID)+\":\"+str(index) in dict_name.keys():\n",
    "            return dict_name[str(sentence_ID)+\":\"+str(index)]\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['sentence_ID', 'verb', 'agent', 'patient']\n",
    "srl_results_df = pd.DataFrame(columns=columns)\n",
    "protag = 'Owl'\n",
    "\n",
    "# for each mention in our coreference cluster do the following\n",
    "for index, row in sentence_ids.iterrows():\n",
    "    token_ID_within_document = row['token_ID_within_document']\n",
    "    #word = df_tokens.loc[df_tokens['token_ID_within_document'] == token_ID_within_document, 'word'].values[0]\n",
    "\n",
    "    sentence_ID = df_tokens.loc[df_tokens['token_ID_within_document'] == token_ID_within_document, 'sentence_ID'].values[0]\n",
    "    words_list = df_tokens.loc[df_tokens['sentence_ID'] == sentence_ID, 'word'].tolist()\n",
    "    token_ID_within_sentence = row['token_ID_within_sentence']\n",
    "    try:\n",
    "        sentence_for_srl = ' '.join(words_list)\n",
    "    except:\n",
    "        print(\"Error: \", words_list)\n",
    "        continue\n",
    "    predictions_srl = predictor_srl.predict(sentence_for_srl)\n",
    "\n",
    "    for n in predictions_srl['verbs']:\n",
    "        if token_ID_within_sentence < len(n['tags']):\n",
    "            tag = n['tags'][token_ID_within_sentence]\n",
    "            if \"B-ARG0\" in tag:\n",
    "\n",
    "                word = check_for_arg(n, \"B-ARG1\", sentence_and_token_ID_to_word_dict, sentence_ID)\n",
    "                if word != None:\n",
    "                    new_row = pd.DataFrame({'sentence_ID': [sentence_ID], 'verb': [utils.lemmatize(n['verb'], nlp)], 'agent': [protag], 'patient': [word]})\n",
    "                    srl_results_df = pd.concat([srl_results_df, new_row], ignore_index=True)\n",
    "            elif \"B-ARG1\" in tag:\n",
    "                word = check_for_arg(n, \"B-ARG0\", sentence_and_token_ID_to_word_dict, sentence_ID)\n",
    "                if word != None:\n",
    "                    new_row = pd.DataFrame({'sentence_ID': [sentence_ID], 'verb': [utils.lemmatize(n['verb'], nlp)], 'agent': [word], 'patient': [protag]})\n",
    "                    srl_results_df = pd.concat([srl_results_df, new_row], ignore_index=True)\n",
    "\n",
    "srl_results_df.to_csv('owl_triples.csv', index=False)\n",
    "del srl_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV saved to SRL_out_winnie/combined_sorted.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Set the path to your folder containing the CSV files\n",
    "folder_path = 'SRL_out_winnie'  \n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "# Read and concatenate all CSV files\n",
    "combined_df = pd.concat((pd.read_csv(f) for f in csv_files), ignore_index=True)\n",
    "\n",
    "# Remove duplicate rows\n",
    "combined_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Sort by 'sentence_id'\n",
    "combined_df.sort_values(by='sentence_ID', inplace=True)\n",
    "\n",
    "# Save the combined, de-duplicated, and sorted DataFrame to a new CSV\n",
    "output_path = os.path.join(folder_path, 'combined_sorted.csv')\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Combined CSV saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV saved to SRL_out_little_princess/combined_sorted.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Set the path to your folder containing the CSV files\n",
    "folder_path = 'SRL_out_little_princess'  # <-- change this\n",
    "\n",
    "# Get a list of all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "# List to collect all processed DataFrames\n",
    "dataframes = []\n",
    "\n",
    "for file in csv_files:\n",
    "    filename = os.path.basename(file)\n",
    "    \n",
    "    # Extract protagonist name using regex\n",
    "    match = re.search(r'_output_(.+?)_little_princess\\.csv$', filename)\n",
    "    if not match:\n",
    "        print(f\"Could not extract protagonist from filename: {filename}\")\n",
    "        continue\n",
    "    # Replace underscores with spaces and capitalize each word\n",
    "    protagonist = match.group(1).replace('_', ' ').title()\n",
    "\n",
    "    # Read CSV\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    # Replace 'protag' in all columns (or limit to one if needed)\n",
    "    df = df.replace('protag', protagonist)\n",
    "\n",
    "    # Add to list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Combine all DataFrames\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Remove duplicates\n",
    "combined_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Sort by 'sentence_id'\n",
    "combined_df.sort_values(by='sentence_ID', inplace=True)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = os.path.join(folder_path, 'combined_sorted.csv')\n",
    "combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Combined CSV saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

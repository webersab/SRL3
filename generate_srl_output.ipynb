{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cpu\n",
      "{'pipeline': 'entity,quote,supersense,event,coref', 'model': 'big'}\n",
      "--- startup: 27.868 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from booknlp.booknlp import BookNLP\n",
    "\n",
    "model_params={\n",
    "\t\t\"pipeline\":\"entity,quote,supersense,event,coref\", \n",
    "\t\t\"model\":\"big\"\n",
    "\t}\n",
    "\t\n",
    "booknlp=BookNLP(\"en\", model_params)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- spacy: 7.132 seconds ---\n",
      "--- entities: 42.034 seconds ---\n",
      "--- quotes: 0.042 seconds ---\n",
      "--- attribution: 86.555 seconds ---\n",
      "--- name coref: 0.047 seconds ---\n",
      "--- coref: 49.948 seconds ---\n",
      "--- TOTAL (excl. startup): 185.894 seconds ---, 81696 words\n"
     ]
    }
   ],
   "source": [
    "# Input file to process\n",
    "input_file=\"a_little_princess.txt\"\n",
    "\n",
    "# Output directory to store resulting files in\n",
    "output_directory=\"./output_dir/princess/\"\n",
    "\n",
    "# File within this directory will be named ${book_id}.entities, ${book_id}.tokens, etc.\n",
    "book_id=\"princess\"\n",
    "\n",
    "booknlp.process(input_file, output_directory, book_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "def proc(filename):\n",
    "    with open(filename) as file:\n",
    "        data=json.load(file)\n",
    "    return data\n",
    "\n",
    "data=proc(\"./output_dir/princess/princess.book\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_counter_from_dependency_list(dep_list):\n",
    "    counter=Counter()\n",
    "    for token in dep_list:\n",
    "        term=token[\"w\"]\n",
    "        tokenGlobalIndex=token[\"i\"]\n",
    "        counter[term]+=1\n",
    "    return counter\n",
    "\n",
    "for character in data[\"characters\"]:\n",
    "\n",
    "    agentList=character[\"agent\"]\n",
    "    patientList=character[\"patient\"]\n",
    "    possList=character[\"poss\"]\n",
    "    modList=character[\"mod\"]\n",
    "\n",
    "    character_id=character[\"id\"]\n",
    "    count=character[\"count\"]\n",
    "\n",
    "    referential_gender_distribution=referential_gender_prediction=\"unknown\"\n",
    "\n",
    "    if character[\"g\"] is not None and character[\"g\"] != \"unknown\":\n",
    "        referential_gender_distribution=character[\"g\"][\"inference\"]\n",
    "        referential_gender=character[\"g\"][\"argmax\"]\n",
    "\n",
    "    mentions=character[\"mentions\"]\n",
    "    proper_mentions=mentions[\"proper\"]\n",
    "    max_proper_mention=\"\"\n",
    "\n",
    "    # just print out information about named characters\n",
    "    if len(mentions[\"proper\"]) > 0:\n",
    "        max_proper_mention=mentions[\"proper\"][0][\"n\"]\n",
    "\n",
    "        print(character_id, count, max_proper_mention, referential_gender)\n",
    "\n",
    "        print()\n",
    "        printTop=100\n",
    "        for k, v in get_counter_from_dependency_list(possList).most_common(printTop):\n",
    "            print(\"\\tposs\\t%s %s\" % (v,k))\n",
    "        print()\n",
    "        for k, v in get_counter_from_dependency_list(agentList).most_common(printTop):\n",
    "            print(\"\\tagent\\t%s %s\" % (v,k))\n",
    "        print()\n",
    "        for k, v in get_counter_from_dependency_list(patientList).most_common(printTop):\n",
    "            print(\"\\tpatient\\t%s %s\" % (v,k))\n",
    "        print()\n",
    "        for k, v in get_counter_from_dependency_list(modList).most_common(printTop):\n",
    "            print(\"\\tmod\\t%s %s\" % (v,k))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tokens = pd.read_csv(\"./output_dir/princess/princess.tokens\", delimiter=\"\\t\")\n",
    "df_entities = pd.read_csv(\"./output_dir/princess/princess.entities\", delimiter=\"\\t\")\n",
    "\n",
    "#create dictionary with the shape [sentence_ID][token_ID_within_sentence][word]\n",
    "sentence_and_token_ID_to_word_dict = {}\n",
    "for character in data[\"characters\"]:\n",
    "    character_id=character[\"id\"]\n",
    "    mentions=character[\"mentions\"]\n",
    "    proper_mentions=mentions[\"proper\"]\n",
    "    if len(mentions[\"proper\"]) > 0:\n",
    "        max_proper_mention=mentions[\"proper\"][0][\"n\"]\n",
    "    else:\n",
    "        max_proper_mention = character_id\n",
    "\n",
    "    unique_start_token_ids = df_entities.loc[df_entities['COREF'] == character_id, 'start_token']\n",
    "    #print(unique_start_token_ids)\n",
    "    sentence_ids = df_tokens[df_tokens['token_ID_within_document'].isin(unique_start_token_ids)][['token_ID_within_document', 'token_ID_within_sentence', 'sentence_ID']]\n",
    "    sentence_ids = sentence_ids.reset_index(drop=True)\n",
    "\n",
    "    for index, row in sentence_ids.iterrows():\n",
    "        sentence_ID = row['sentence_ID']\n",
    "        token_ID = row['token_ID_within_sentence']\n",
    "        combined_sentence_and_token_ID = str(sentence_ID) +\":\"+ str(token_ID)\n",
    "        sentence_and_token_ID_to_word_dict[combined_sentence_and_token_ID] = max_proper_mention\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235       1249\n",
      "247       1308\n",
      "285       1594\n",
      "287       1600\n",
      "288       1604\n",
      "         ...  \n",
      "12707    80512\n",
      "12708    80519\n",
      "12709    80525\n",
      "12717    80585\n",
      "12719    80605\n",
      "Name: start_token, Length: 1084, dtype: int64\n",
      "      token_ID_within_document  token_ID_within_sentence  sentence_ID\n",
      "0                         1249                        38           47\n",
      "1                         1308                        11           50\n",
      "2                         1594                         5           65\n",
      "3                         1600                         0           66\n",
      "4                         1604                         4           66\n",
      "...                        ...                       ...          ...\n",
      "1079                     80512                         4         4425\n",
      "1080                     80519                        11         4425\n",
      "1081                     80525                        17         4425\n",
      "1082                     80585                        10         4427\n",
      "1083                     80605                        12         4428\n",
      "\n",
      "[1084 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "character = data[\"characters\"][1]\n",
    "character_id=character[\"id\"]\n",
    "\n",
    "#get all sentencs where COREF = character_id\n",
    "unique_start_token_ids = df_entities.loc[df_entities['COREF'] == character_id, 'start_token']\n",
    "print(unique_start_token_ids)\n",
    "sentence_ids = df_tokens[df_tokens['token_ID_within_document'].isin(unique_start_token_ids)][['token_ID_within_document', 'token_ID_within_sentence', 'sentence_ID']]\n",
    "sentence_ids = sentence_ids.reset_index(drop=True)\n",
    "print(sentence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "SRL_MODEL_PATH = \"srl.tar.gz\"\n",
    "predictor_srl = Predictor.from_path(SRL_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_arg(n, arg_name, dict_name, sentence_ID):\n",
    "    if arg_name in n[\"tags\"]:\n",
    "        index = n['tags'].index(arg_name)\n",
    "        if str(sentence_ID)+\":\"+str(index) in dict_name.keys():\n",
    "            return dict_name[str(sentence_ID)+\":\"+str(index)]\n",
    "        else:\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['sentence_ID', 'verb', 'agent', 'patient']\n",
    "srl_results_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# for each mention in our coreference cluster do the following\n",
    "for index, row in sentence_ids.iterrows():\n",
    "    token_ID_within_document = row['token_ID_within_document']\n",
    "    #word = df_tokens.loc[df_tokens['token_ID_within_document'] == token_ID_within_document, 'word'].values[0]\n",
    "\n",
    "    sentence_ID = df_tokens.loc[df_tokens['token_ID_within_document'] == token_ID_within_document, 'sentence_ID'].values[0]\n",
    "    words_list = df_tokens.loc[df_tokens['sentence_ID'] == sentence_ID, 'word'].tolist()\n",
    "    token_ID_within_sentence = row['token_ID_within_sentence']\n",
    "    sentence_for_srl = ' '.join(words_list)\n",
    "    predictions_srl = predictor_srl.predict(sentence_for_srl)\n",
    "\n",
    "    for n in predictions_srl['verbs']:\n",
    "        if token_ID_within_sentence < len(n['tags']):\n",
    "            tag = n['tags'][token_ID_within_sentence]\n",
    "            if \"B-ARG0\" in tag:\n",
    "\n",
    "                word = check_for_arg(n, \"B-ARG1\", sentence_and_token_ID_to_word_dict, sentence_ID)\n",
    "                if word != None:\n",
    "                    new_row = pd.DataFrame({'sentence_ID': [sentence_ID], 'verb': [utils.lemmatize(n['verb'], nlp)], 'agent': [\"protag\"], 'patient': [word]})\n",
    "                    srl_results_df = pd.concat([srl_results_df, new_row], ignore_index=True)\n",
    "            elif \"B-ARG1\" in tag:\n",
    "                word = check_for_arg(n, \"B-ARG0\", sentence_and_token_ID_to_word_dict, sentence_ID)\n",
    "                if word != None:\n",
    "                    new_row = pd.DataFrame({'sentence_ID': [sentence_ID], 'verb': [utils.lemmatize(n['verb'], nlp)], 'agent': [word], 'patient': [\"protag\"]})\n",
    "                    srl_results_df = pd.concat([srl_results_df, new_row], ignore_index=True)\n",
    "\n",
    "srl_results_df.to_csv('srl_output_princess.csv', index=False)\n",
    "del srl_results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
